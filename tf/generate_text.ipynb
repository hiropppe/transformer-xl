{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "from gpu_utils import assign_to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\"model_dir\", default='./EXP-natsume/',\n",
    "      help=\"Estimator model_dir.\")\n",
    "\n",
    "flags.DEFINE_string(\"eval_ckpt_path\", None, '')\n",
    "\n",
    "flags.DEFINE_string(\"spm_file\", '../data/natsume/natsume.model', '')\n",
    "flags.DEFINE_integer(\"num_generate\", 30, '')\n",
    "\n",
    "# Model config\n",
    "flags.DEFINE_integer(\"tgt_len\", default=1,\n",
    "      help=\"Number of steps to predict\")\n",
    "flags.DEFINE_integer(\"mem_len\", default=640,\n",
    "      help=\"Number of steps to cache\")\n",
    "flags.DEFINE_bool(\"same_length\", default=True,\n",
    "      help=\"Same length attention\")\n",
    "flags.DEFINE_integer(\"clamp_len\", default=400,\n",
    "      help=\"Clamp length\")\n",
    "\n",
    "flags.DEFINE_integer(\"n_layer\", default=16,\n",
    "      help=\"Number of layers.\")\n",
    "flags.DEFINE_integer(\"d_model\", default=410,\n",
    "      help=\"Dimension of the model.\")\n",
    "flags.DEFINE_integer(\"d_embed\", default=410,\n",
    "      help=\"Dimension of the embeddings.\")\n",
    "flags.DEFINE_integer(\"n_head\", default=10,\n",
    "      help=\"Number of attention heads.\")\n",
    "flags.DEFINE_integer(\"d_head\", default=41,\n",
    "      help=\"Dimension of each attention head.\")\n",
    "flags.DEFINE_integer(\"d_inner\", default=2100,\n",
    "      help=\"Dimension of inner hidden size in positionwise feed-forward.\")\n",
    "flags.DEFINE_float(\"dropout\", default=0.0,\n",
    "      help=\"Dropout rate.\")\n",
    "flags.DEFINE_float(\"dropatt\", default=0.0,\n",
    "      help=\"Attention dropout rate.\")\n",
    "flags.DEFINE_bool(\"untie_r\", default=True,\n",
    "      help=\"untie r_w_bias and r_r_bias\")\n",
    "\n",
    "# Adaptive Softmax / Embedding\n",
    "flags.DEFINE_bool(\"tie_weight\", default=True,\n",
    "      help=\"Tie embedding and softmax weight.\")\n",
    "flags.DEFINE_integer(\"div_val\", default=1,\n",
    "      help=\"Divide the embedding size by this val for each bin\")\n",
    "flags.DEFINE_bool(\"proj_share_all_but_first\", default=True,\n",
    "      help=\"True to share all but first projs, False not to share.\")\n",
    "flags.DEFINE_bool(\"proj_same_dim\", default=True,\n",
    "      help=\"Project the bin with the same dimension.\")\n",
    "\n",
    "# Parameter initialization\n",
    "flags.DEFINE_enum(\"init\", default=\"normal\",\n",
    "      enum_values=[\"normal\", \"uniform\"],\n",
    "      help=\"Initialization method.\")\n",
    "flags.DEFINE_float(\"init_std\", default=0.02,\n",
    "      help=\"Initialization std when init is normal.\")\n",
    "flags.DEFINE_float(\"proj_init_std\", default=0.01,\n",
    "      help=\"Initialization std for embedding projection.\")\n",
    "flags.DEFINE_float(\"init_range\", default=0.1,\n",
    "      help=\"Initialization std when init is uniform.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_fn(n_token, cutoffs):\n",
    "  def model_fn(inp, tgt, mems, is_training):\n",
    "    inp = tf.transpose(inp, [1, 0])\n",
    "\n",
    "    if FLAGS.init == \"uniform\":\n",
    "      initializer = tf.initializers.random_uniform(\n",
    "          minval=-FLAGS.init_range,\n",
    "          maxval=FLAGS.init_range,\n",
    "          seed=None)\n",
    "    elif FLAGS.init == \"normal\":\n",
    "      initializer = tf.initializers.random_normal(\n",
    "          stddev=FLAGS.init_std,\n",
    "          seed=None)\n",
    "      proj_initializer = tf.initializers.random_normal(\n",
    "          stddev=FLAGS.proj_init_std,\n",
    "          seed=None)\n",
    "\n",
    "    tie_projs = [False for _ in range(len(cutoffs) + 1)]\n",
    "    if FLAGS.proj_share_all_but_first:\n",
    "      for i in range(1, len(tie_projs)):\n",
    "        tie_projs[i] = True\n",
    "\n",
    "    probs = model.decode(\n",
    "        dec_inp=inp,\n",
    "        mems=mems,\n",
    "        n_token=n_token,\n",
    "        n_layer=FLAGS.n_layer,\n",
    "        d_model=FLAGS.d_model,\n",
    "        d_embed=FLAGS.d_embed,\n",
    "        n_head=FLAGS.n_head,\n",
    "        d_head=FLAGS.d_head,\n",
    "        d_inner=FLAGS.d_inner,\n",
    "        dropout=FLAGS.dropout,\n",
    "        dropatt=FLAGS.dropatt,\n",
    "        initializer=initializer,\n",
    "        proj_initializer=proj_initializer,\n",
    "        is_training=is_training,\n",
    "        mem_len=FLAGS.mem_len,\n",
    "        cutoffs=cutoffs,\n",
    "        div_val=FLAGS.div_val,\n",
    "        tie_projs=tie_projs,\n",
    "        input_perms=None,\n",
    "        target_perms=None,\n",
    "        head_target=None,\n",
    "        same_length=FLAGS.same_length,\n",
    "        clamp_len=FLAGS.clamp_len,\n",
    "        use_tpu=False,\n",
    "        untie_r=FLAGS.untie_r,\n",
    "        proj_same_dim=FLAGS.proj_same_dim)\n",
    "\n",
    "    # number of parameters\n",
    "    num_params = sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
    "    tf.logging.info('#params: {}'.format(num_params))\n",
    "\n",
    "    # format_str = '{{:<{0}s}}\\t{{}}'.format(\n",
    "    #     max([len(v.name) for v in tf.trainable_variables()]))\n",
    "    # for v in tf.trainable_variables():\n",
    "    #   tf.logging.info(format_str.format(v.name, v.get_shape()))\n",
    "    return probs\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_core_graph(n_token, cutoffs, is_training, inp, tgt, mems):\n",
    "  model_fn = get_model_fn(\n",
    "      n_token=n_token,\n",
    "      cutoffs=cutoffs)\n",
    "\n",
    "  model_ret = model_fn(\n",
    "      inp=inp,\n",
    "      tgt=tgt,\n",
    "      mems=mems,\n",
    "      is_training=is_training)\n",
    "\n",
    "  return model_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(FLAGS.spm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:#params: 44367520\n"
     ]
    }
   ],
   "source": [
    "tower_mems = []\n",
    "\n",
    "with tf.device(assign_to_gpu(0, \"/gpu:0\")), \\\n",
    "     tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "  inp_ph = tf.placeholder(tf.int32, [1, None])\n",
    "\n",
    "  mems_i = [tf.placeholder(tf.float32, [FLAGS.mem_len, 1, FLAGS.d_model])\n",
    "            for _ in range(FLAGS.n_layer)]\n",
    "\n",
    "  prob = single_core_graph(\n",
    "    n_token=sp.get_piece_size(),\n",
    "    cutoffs=[],\n",
    "    is_training=False,\n",
    "    inp=inp_ph,\n",
    "    tgt=None,\n",
    "    mems=mems_i)\n",
    "\n",
    "  tower_mems.append(mems_i)\n",
    "\n",
    "tower_mems_np = [\n",
    "  [np.zeros([FLAGS.mem_len, 1, FLAGS.d_model], dtype=np.float32)\n",
    "    for layer in range(FLAGS.n_layer)]\n",
    "  for core in range(1)\n",
    "]\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パプリカ(6 2639 4087 878 1048)\n"
     ]
    }
   ],
   "source": [
    "start_string = 'パプリカ'\n",
    "\n",
    "start_ids = sp.encode_as_ids(start_string)\n",
    "print('{:s}({:s})'.format(start_string, ' '.join(str(i) for i in start_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['野球', '試合', '東京ドーム']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./EXP-natsume/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if FLAGS.eval_ckpt_path is None:\n",
    "  eval_ckpt_path = tf.train.latest_checkpoint(FLAGS.model_dir)\n",
    "else:\n",
    "  eval_ckpt_path = FLAGS.eval_ckpt_path\n",
    "saver.restore(sess, eval_ckpt_path)\n",
    "\n",
    "feed_dict = {}\n",
    "for m, m_np in zip(tower_mems[0], tower_mems_np[0]):\n",
    "  feed_dict[m] = m_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ 猫 といえども 相当の 猫 を 鼓 吹 する に相違ない 。 ▁ 御母さん は 、 今 ここ が 平生の 通り 落ち 付いて 来て 、 御前の 顔は さ ば か に 心得ている 。\n"
     ]
    }
   ],
   "source": [
    "start_string = '猫'\n",
    "start_ids = sp.encode_as_ids(start_string)\n",
    "ids = []\n",
    "ids.extend(start_ids)\n",
    "\n",
    "fetches = [prob]\n",
    "\n",
    "for i in range(FLAGS.num_generate):\n",
    "    feed_dict[inp_ph] = np.expand_dims(ids, 0)\n",
    "    fetched = sess.run(fetches, feed_dict=feed_dict)\n",
    "    predictions = fetched[0]\n",
    "    predictions = np.squeeze(predictions[-1], 0)\n",
    "    predicted_id = int(np.argmax(predictions))\n",
    "    ids.append(predicted_id)\n",
    "    #print(' '.join([str(i) for i in ids]))\n",
    "\n",
    "print(' '.join(sp.id_to_piece(i) for i in ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(prev_ids, text=None):\n",
    "    ids = []\n",
    "    ids.extend(prev_ids)\n",
    "    if text:\n",
    "      extra_ids = sp.encode_as_ids(text)\n",
    "      if extra_ids[0] == 6:\n",
    "        extra_ids = extra_ids[1:]\n",
    "      ids.extend(extra_ids)\n",
    "    feed_dict[inp_ph] = np.expand_dims(ids, 0)\n",
    "    fetched = sess.run(fetches, feed_dict=feed_dict)\n",
    "    preds = fetched[0]\n",
    "    preds = np.squeeze(preds[-1], 0)\n",
    "    pred_id = int(np.argmax(preds))\n",
    "    ids.append(pred_id)\n",
    "    print(' '.join(sp.id_to_piece(i) for i in ids))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁ 猫 といえども\n",
      "▁ 猫 といえども 相 当 する\n",
      "▁ 猫 といえども 相 当 する 。\n"
     ]
    }
   ],
   "source": [
    "start_string = '猫'\n",
    "start_ids = sp.encode_as_ids(start_string)\n",
    "new_ids = gen(start_ids)\n",
    "new_ids = gen(new_ids, '相当')\n",
    "new_ids = gen(new_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
